{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emmanuelarchie/Detecting-Toxicity-in-Text-emailing-systems-/blob/main/Copy_of_Detecting_Toxicity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate\n"
      ],
      "metadata": {
        "id": "6t3v8J_ylMX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKVjbJaKYRM1"
      },
      "outputs": [],
      "source": [
        "# Basic dataset information\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"train_updated.csv\")\n",
        "df = df[df['clean_text'].str.strip() != \"\"]\n",
        "\n",
        "# Show basic info\n",
        "df.info()\n",
        "\n",
        "# Show first few rows\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9y9nu23Y88r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"toxic_cleaned.csv\")\n",
        "df = df[df['clean_text'].str.strip() != \"\"]\n",
        "\n",
        "# Check for null values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check class distribution\n",
        "print(df['toxic'].value_counts())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Map numeric labels to class names\n",
        "label_names = {0: 'Non-toxic', 1: 'Toxic'}\n",
        "label_counts = df['toxic'].map(label_names).value_counts()\n",
        "\n",
        "# Plot with custom labels\n",
        "label_counts.plot(kind='bar', title='Original Class Distribution (Toxic vs Non-toxic)', color=['skyblue', 'salmon'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"train_updated.csv\")\n",
        "\n",
        "# Ensure clean data\n",
        "df = df.dropna(subset=[\"clean_text\", \"labels\"])\n",
        "\n",
        "# Split into toxic and non-toxic\n",
        "toxic_df = df[df[\"labels\"] == 1]\n",
        "non_toxic_df = df[df[\"labels\"] == 0]\n",
        "\n",
        "# Downsample non-toxic to match toxic size\n",
        "non_toxic_sampled = non_toxic_df.sample(n=2*len(toxic_df), random_state=42)\n",
        "\n",
        "# Combine\n",
        "balanced_df = pd.concat([toxic_df, non_toxic_sampled]).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save or use this\n",
        "balanced_df.to_csv(\"balanced_train.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Balanced dataset created!\")\n",
        "print(\"Class distribution:\\n\", balanced_df[\"labels\"].value_counts())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Map numeric labels to class names\n",
        "label_names = {0: 'Non-toxic', 1: 'Toxic'}\n",
        "label_counts = balanced_df['labels'].map(label_names).value_counts()\n",
        "\n",
        "# Plot with custom labels\n",
        "label_counts.plot(kind='bar', title='Balanced Class Distribution (Toxic vs Non-toxic)', color=['skyblue', 'salmon'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MUU7DxgiaDJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E6quUEuepEj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    # Remove standard URLs (http, https, www)\n",
        "    text = re.sub(r\"(https?:\\/\\/|www\\.)\\S+\", \"\", text)\n",
        "    # Remove path-like or pseudo-URL strings: things with multiple slashes or colons\n",
        "    text = re.sub(r\"\\(?\\b(?:[a-z0-9]+[:/])+[a-z0-9/]+\\)?\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)          # remove mentions\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)          # remove hashtags\n",
        "    text = re.sub(r\"\\d{1,2}:\\d{2}\", \"\", text)  # remove times like 21:51\n",
        "    text = re.sub(r\"\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2},\\s*\\d{4}(?:\\s*\\(utc\\))?\",\"\", text)\n",
        "    text = re.sub(r\"\\d{4}\", \"\", text)          # remove years like 2016\n",
        "\n",
        "    # Remove IP addresses with optional ports (e.g., 70.100.229.154 :57 or 192.168.1.1:8080)\n",
        "    #text = re.sub(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\s*:?\\d*\\b\", \"\", text)\n",
        "    text = re.sub(r\"(?<!\\d)(?:(?:\\d{1,3}\\.){3}\\d{1,3})(?!\\d)\", \"\", text)\n",
        "\n",
        "    # Remove strange encoding characters like Ã¢â‚¬Â¢ and other non-ASCII characters\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "\n",
        "    #text = re.sub(r\"\\d+\", \"\", text)            # remove other numbers\n",
        "    #text = re.sub(r\"[^a-z\\s]\", \"\", text)       # remove non-letter characters, optional\n",
        "    text = re.sub(r\"\\s+\", \" \", text)           # collapse whitespace\n",
        "\n",
        "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
        "    return text.strip()\n",
        "\n",
        "#df['clean_text'] = df['comment_text'].apply(clean_text)\n",
        "df['clean_text'] = df['clean_text'].apply(clean_text)\n",
        "\n",
        "#df = df[['clean_text', 'toxic']]\n",
        "df.to_csv(\"train_updated.csv\", index=False)\n",
        "#print(repr(df['comment_text'].iloc[1]))       # before\n",
        "#print(repr(df['clean_text'].iloc[1]))         # after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39N22TUjk9K8"
      },
      "outputs": [],
      "source": [
        "#Pre trained model evaluation\n",
        "import pandas as pd\n",
        "\n",
        "# Load your cleaned dataset\n",
        "test_df = pd.read_csv(\"balanced_train.csv\")\n",
        "\n",
        "# Check class distribution\n",
        "print(test_df['labels'].value_counts())\n",
        "\n",
        "texts = test_df[\"clean_text\"].tolist()\n",
        "true_labels = test_df[\"labels\"].tolist()\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"s-nlp/roberta_toxicity_classifier\",\n",
        "    device=0,  # ensures model runs on GPU\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "def clean_text_input(t):\n",
        "    return str(t).strip() if isinstance(t, str) else \"\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "preds = []\n",
        "\n",
        "batch_size = 32\n",
        "for i in tqdm(range(0, len(texts), batch_size)):\n",
        "    batch = texts[i:i + batch_size]\n",
        "\n",
        "    # Clean but don't skip â€” preserve batch size\n",
        "    clean_batch = [clean_text_input(text) for text in batch]\n",
        "\n",
        "    # Get predictions\n",
        "    results = classifier(clean_batch)\n",
        "\n",
        "    # In case fewer outputs are returned, pad with neutral (safe fallback)\n",
        "    if len(results) < len(batch):\n",
        "        results += [{'label': 'neutral', 'score': 1.0}] * (len(batch) - len(results))\n",
        "\n",
        "    batch_preds = [1 if r['label'].lower() == 'toxic' else 0 for r in results]\n",
        "    preds.extend(batch_preds)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, preds, target_names=[\"non-toxic\", \"toxic\"]))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, preds))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(true_labels, preds)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"non-toxic\", \"toxic\"], yticklabels=[\"non-toxic\", \"toxic\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix - s-nlp/roberta_toxicity_classifier\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHhllzQgvdiD"
      },
      "outputs": [],
      "source": [
        "#Paraphrasing to augment toxic class in the dataset\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "#torch.cuda.reset_peak_memory_stats()\n",
        "print(\"GPU available?\", torch.cuda.is_available())\n",
        "\n",
        "# Load the cleaned 5-fold dataset you created earlier\n",
        "df = pd.read_csv(\"toxic_cleaned.csv\")\n",
        "\n",
        "# Filter toxic rows\n",
        "toxic_df = df[df[\"toxic\"] == 1].copy()\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# Load paraphraser model (T5)\n",
        "paraphraser = pipeline(\"text2text-generation\", model=\"Vamsi/T5_Paraphrase_Paws\", device=0)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "toxic_texts = toxic_df[\"clean_text\"].sample(20000, replace=True, random_state=42).tolist()\n",
        "from datasets import Dataset\n",
        "\n",
        "# Sample and clean 20,000 toxic rows\n",
        "sampled_toxic = toxic_df[\"clean_text\"].sample(20000, replace=True, random_state=42).apply(clean_text).tolist()\n",
        "\n",
        "# Create Hugging Face Dataset\n",
        "paraphrase_dataset = Dataset.from_dict({\"text\": [f\"paraphrase: {t}\" for t in sampled_toxic]})\n",
        "\n",
        "# Generate paraphrased outputs in batch\n",
        "outputs = paraphraser(paraphrase_dataset[\"text\"], batch_size=16, max_length=512, truncation=True)\n",
        "\n",
        "# Extract just the generated text\n",
        "paraphrased = [item[\"generated_text\"].strip() for item in outputs]\n",
        "\n",
        "\n",
        "# Convert to DataFrame\n",
        "paraphrased_df = pd.DataFrame({\n",
        "    \"clean_text\": paraphrased,\n",
        "    \"toxic\": 1,\n",
        "    \"augmented\": \"paraphrased\"\n",
        "})\n",
        "\n",
        "\n",
        "paraphrased_df.to_csv(\"paraphrased_toxic_comments.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"paraphrased_toxic_comments.csv\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training bert-base-uncased model on our dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"balanced_train.csv\")  # Ensure it has 'text' and 'label' columns\n",
        "df = df.dropna(subset=[\"clean_text\", \"labels\"])  # Clean missing values\n",
        "\n",
        "\n",
        "model_name = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        ")\n",
        "\n",
        "print(model.config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define metrics\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "best_f1 = 0\n",
        "best_fold = 0\n",
        "f1_scores = []\n",
        "confusion_matrices = []\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"clean_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "\n",
        "    print(f\"logits shape: {logits.shape}, labels shape: {labels.shape}, labels dtype: {labels.dtype}\")\n",
        "\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "\n",
        "    # Macro F1 (average over both classes)\n",
        "    macro_f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "\n",
        "    # Binary F1, Precision, Recall (focus on toxic class)\n",
        "    binary_f1 = f1_score(labels, preds, pos_label=1)\n",
        "    precision = precision_score(labels, preds, pos_label=1)\n",
        "    recall = recall_score(labels, preds, pos_label=1)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Toxic\", \"Toxic\"], yticklabels=[\"Non-Toxic\", \"Toxic\"])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Save scores\n",
        "    f1_scores.append(binary_f1)\n",
        "    confusion_matrices.append(cm)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1_macro\": macro_f1,\n",
        "        \"f1_toxic\": binary_f1,\n",
        "        \"precision_toxic\": precision,\n",
        "        \"recall_toxic\": recall\n",
        "    }\n",
        "# Convert to Hugging Face dataset format\n",
        "all_texts = df[\"clean_text\"].tolist()\n",
        "all_labels = df[\"labels\"].astype(int).tolist()\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "fold = 1\n",
        "for train_index, val_index in skf.split(all_texts, all_labels):\n",
        "    print(f\"\\nðŸ” Fold {fold}\")\n",
        "\n",
        "    train_df = pd.DataFrame({\n",
        "        \"clean_text\": [all_texts[i] for i in train_index],\n",
        "        \"labels\": [all_labels[i] for i in train_index]\n",
        "    })\n",
        "\n",
        "    val_df = pd.DataFrame({\n",
        "        \"clean_text\": [all_texts[i] for i in val_index],\n",
        "        \"labels\": [all_labels[i] for i in val_index]\n",
        "    })\n",
        "\n",
        "\n",
        "    # Convert to Hugging Face datasets and tokenize\n",
        "    train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True).remove_columns([\"clean_text\"])\n",
        "    val_dataset = Dataset.from_pandas(val_df).map(tokenize, batched=True).remove_columns([\"clean_text\"])\n",
        "\n",
        "    print(train_dataset[0])\n",
        "    print(val_dataset[0])\n",
        "\n",
        "    # Training configuration\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_fold_{fold}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f\"./logs_fold_{fold}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_f1_toxic\",\n",
        "        logging_steps=50,\n",
        "        save_total_limit=1\n",
        "    )\n",
        "\n",
        "    # Trainer setup\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    for batch in trainer.get_train_dataloader():\n",
        "      print(batch['labels'].shape, batch['labels'].dtype)\n",
        "      break\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = trainer.evaluate()\n",
        "    if metrics[\"eval_f1_toxic\"] > best_f1:\n",
        "      best_f1 = metrics[\"eval_f1_toxic\"]\n",
        "      best_fold = fold\n",
        "      trainer.save_model(f\"./best_model\")\n",
        "      tokenizer.save_pretrained(f\"./best_model\")\n",
        "\n",
        "    print(f\"ðŸ“Š Fold {fold} Evaluation Metrics:\", metrics)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "print(\"Average Toxic F1 Score:\", np.mean(f1_scores))\n",
        "\n",
        "# Optionally, sum confusion matrices:\n",
        "total_cm = sum(confusion_matrices)\n",
        "print(\"Total Confusion Matrix:\\n\", total_cm)\n",
        "\n",
        "\n",
        "# Save to text file\n",
        "with open(\"final_results.txt\", \"w\") as f:\n",
        "    f.write(f\"Average Toxic F1 Score: {np.mean(f1_scores):.4f}\\n\\n\")\n",
        "    f.write(\"Total Confusion Matrix:\\n\")\n",
        "    for row in total_cm:\n",
        "        f.write(\" \".join(map(str, row)) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "lWWwq7eH8-zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training distilroberta-base model on our dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"balanced_train.csv\")  # Ensure it has 'text' and 'label' columns\n",
        "df = df.dropna(subset=[\"clean_text\", \"labels\"])  # Clean missing values\n",
        "\n",
        "\n",
        "model_name = \"distilbert/distilroberta-base\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        ")\n",
        "\n",
        "print(model.config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define metrics\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "best_f1 = 0\n",
        "best_fold = 0\n",
        "f1_scores = []\n",
        "confusion_matrices = []\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"clean_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "\n",
        "    print(f\"logits shape: {logits.shape}, labels shape: {labels.shape}, labels dtype: {labels.dtype}\")\n",
        "\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "\n",
        "    # Macro F1 (average over both classes)\n",
        "    macro_f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "\n",
        "    # Binary F1, Precision, Recall (focus on toxic class)\n",
        "    binary_f1 = f1_score(labels, preds, pos_label=1)\n",
        "    precision = precision_score(labels, preds, pos_label=1)\n",
        "    recall = recall_score(labels, preds, pos_label=1)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Toxic\", \"Toxic\"], yticklabels=[\"Non-Toxic\", \"Toxic\"])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Save scores\n",
        "    f1_scores.append(binary_f1)\n",
        "    confusion_matrices.append(cm)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1_macro\": macro_f1,\n",
        "        \"f1_toxic\": binary_f1,\n",
        "        \"precision_toxic\": precision,\n",
        "        \"recall_toxic\": recall\n",
        "    }\n",
        "# Convert to Hugging Face dataset format\n",
        "all_texts = df[\"clean_text\"].tolist()\n",
        "all_labels = df[\"labels\"].astype(int).tolist()\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "fold = 1\n",
        "for train_index, val_index in skf.split(all_texts, all_labels):\n",
        "    print(f\"\\nðŸ” Fold {fold}\")\n",
        "\n",
        "    train_df = pd.DataFrame({\n",
        "        \"clean_text\": [all_texts[i] for i in train_index],\n",
        "        \"labels\": [all_labels[i] for i in train_index]\n",
        "    })\n",
        "\n",
        "    val_df = pd.DataFrame({\n",
        "        \"clean_text\": [all_texts[i] for i in val_index],\n",
        "        \"labels\": [all_labels[i] for i in val_index]\n",
        "    })\n",
        "\n",
        "\n",
        "    # Convert to Hugging Face datasets and tokenize\n",
        "    train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True).remove_columns([\"clean_text\"])\n",
        "    val_dataset = Dataset.from_pandas(val_df).map(tokenize, batched=True).remove_columns([\"clean_text\"])\n",
        "\n",
        "    print(train_dataset[0])\n",
        "    print(val_dataset[0])\n",
        "\n",
        "    # Training configuration\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_fold_{fold}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f\"./logs_fold_{fold}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_f1_toxic\",\n",
        "        logging_steps=50,\n",
        "        save_total_limit=1\n",
        "    )\n",
        "\n",
        "    # Trainer setup\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    for batch in trainer.get_train_dataloader():\n",
        "      print(batch['labels'].shape, batch['labels'].dtype)\n",
        "      break\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = trainer.evaluate()\n",
        "    if metrics[\"eval_f1_toxic\"] > best_f1:\n",
        "      best_f1 = metrics[\"eval_f1_toxic\"]\n",
        "      best_fold = fold\n",
        "      trainer.save_model(f\"./best_model\")\n",
        "      tokenizer.save_pretrained(f\"./best_model\")\n",
        "\n",
        "    print(f\"ðŸ“Š Fold {fold} Evaluation Metrics:\", metrics)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "print(\"Average Toxic F1 Score:\", np.mean(f1_scores))\n",
        "\n",
        "# Optionally, sum confusion matrices:\n",
        "total_cm = sum(confusion_matrices)\n",
        "print(\"Total Confusion Matrix:\\n\", total_cm)\n",
        "\n",
        "\n",
        "# Save to text file\n",
        "with open(\"final_results.txt\", \"w\") as f:\n",
        "    f.write(f\"Average Toxic F1 Score: {np.mean(f1_scores):.4f}\\n\\n\")\n",
        "    f.write(\"Total Confusion Matrix:\\n\")\n",
        "    for row in total_cm:\n",
        "        f.write(\" \".join(map(str, row)) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "Om6i76vRreH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = [[69395, 1187], [591, 34724]]\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[\"Non-Toxic\", \"Toxic\"],\n",
        "            yticklabels=[\"Non-Toxic\", \"Toxic\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Total Confusion Matrix - distilbert/distilroberta-base\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ImaM730K_tBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}